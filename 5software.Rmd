
```{r setup04, include=FALSE}
require(knitr)
require(glmnet)
require(kableExtra)
require(dplyr)
require(xgboost)
require(SuperLearner)
require(sl3)
require(Rsolnp)
require(ltmle)
require(AIPW)
require(tmle3)
options(knitr.kable.NA = '')
cachex=TRUE
```

```{r dataload_01, cache=cachex, echo = TRUE}
# Read the data saved at the last chapter
ObsData <- readRDS(file = "data/rhcAnalytic.RDS")
dim(ObsData)
```

# Pre-packaged software

## sl3
The _sl3_ package implements two types of Super Learning: discrete Super Learning, in which the best prediction algorithm (based on cross-validation) from a specified library is returned, and ensemble Super Learning, in which the best linear combination of the specified algorithms is returned (@coyle2021sl3).

The first step is to create a sl3 task which keeps track of the roles of the variables in our problem (@coyle2021tlverse). 

```{r sl3_01}
# create sl3 task, specifying outcome and covariates 
rhc_task <- make_sl3_Task(
  data = ObsData, 
  covariates = colnames(ObsData)[-which(names(ObsData) == "Y")],
  outcome = "Y"
)
print(rhc_task)
```

Next, we create our SuperLearner. To do this, we need to specify a selection of machine learning algorithms we want to include as candidates, as well as a metalearner that the SuperLearner will use to combine or choose from the machine learning algorithms provided (@coyle2021tlverse). 
```{r sl3_02}
# see what algorithms are available for a continuous outcome (similar can be done for a binary outcome)
sl3_list_learners("continuous")
```

The chosen candidate algorithms can be created and collected in a Stack.
```{r sl3_03}
# initialize candidate learners
lrn_glm <- make_learner(Lrnr_glm)
lrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1
xgb_50 <- Lrnr_xgboost$new(nrounds = 50)

# collect learners in stack
stack <- make_learner(
  Stack, lrn_glm, lrn_lasso, xgb_50
)
```

The stack is then given to the SuperLearner.
```{r sl3_04}
# to make an ensemble SuperLearner
sl <- make_learner(Lrnr_sl, learners = stack)

# or a discrete SuperLearner
sl_disc_meta <- Lrnr_cv_selector$new()
sl_disc <- Lrnr_sl$new(
  learners = stack, 
  metalearner = sl_disc_meta
)
```

The SuperLearner is then trained on the sl3 task we created at the start and then it can be used to make predictions.
```{r sl3_05}
set.seed(1444)

# train SL
sl_fit <- sl$train(rhc_task)
# or for discrete SL
# sl_fit <- sl_disc$train(rhc_task)

# make predictions
sl3_data <- ObsData
sl3_data$sl_preds <- sl_fit$predict()

sl3_est <- mean(sl3_data$Y[sl3_data$A == 1]) - mean(sl3_data$Y[sl3_data$A == 0])
print(sl3_est)

# TODO
sl3_ci <- paste("(", NA, ", ", NA, ")", sep = "")
```


## tmle
The _tmle_ package can handle both binary and continuous outcomes, and uses the _SuperLearner_ package to construct both models just like we did in the steps above. The default SuperLearner library for estimating the outcome includes generalized linear models (GLMs), GLM with elastic net regularization, and Bayesian additive regression trees. The default library for estimating the propensity scores also includes GLMs and Bayesian additive regression trees (though specified slightly differently), and replaces the GLM with regularization with generalized additive models (GAMs) (@tmlePkgDocs). More methods can be added by specifying lists of models in the _Q.SL.library_ (for the outcome model) and _g.SL.library_ (for the propensity score model) arguments. Note also that the outcome $Y$ is required to be within the range of $[0,1]$ for this method as well, so we need to pass in the transformed data, then transform back the estimate.

```{r tmle_pkg, cache=TRUE}
set.seed(1444) 

# transform the outcome to fall within the range [0,1]
a <- min(na.omit(ObsData$Y))
b <- max(na.omit(ObsData$Y))
ObsData$Y_transf <- (ObsData$Y-a)/(b-a)

# run tmle from the tmle package 
W <- dplyr::select(ObsData, !c(Y_transf, Y, A))
SL.library = c("SL.glm", "SL.glmnet", "SL.xgboost")
tmle <- tmle::tmle(Y = ObsData$Y_transf, A = ObsData$A, W = W, family = "gaussian", Q.SL.library = SL.library, g.SL.library = SL.library)
tmle_est_tr <- tmle$estimates$ATE$psi
# transform back the ATE estimate
tmle_est <- (b-a)*tmle_est_tr

tmle_ci <- paste("(", round((b-a)*tmle$estimates$ATE$CI[1], 3), ", ", round((b-a)*tmle$estimates$ATE$CI[2], 3), ")", sep = "")

cat("ATE from tmle package: ", tmle_est, tmle_ci, sep = "")
```

## ltmle
Similarly to the _tmle_ package, the _ltmle_ package gives the direct TMLE result with the call of one function. 

```{r ltmle_pkg, cache=TRUE}
# exclude Y_transf since ltmle scales automatically
ltmle_data <- dplyr::select(ObsData, !Y_transf) 

# run ltmle
ltmle_est <- ltmle(ltmle_data, Anodes = "A", Ynodes = "Y", abar = 1, SL.library = SL.library)

# print result & confidence intervals
ltmle_ci <- paste("(", round(summary(ltmle_est)[["treatment"]][["CI"]][,"2.5%"], 3), ", ", round(summary(ltmle_est)[["treatment"]][["CI"]][,"97.5%"], 3), ")", sep = "")
cat("ATE from ltmle package: ", ltmle_est$estimates[["tmle"]], ltmle_ci, sep = "")
```
The main difference between the _tmle_ and the _ltmle_ package is that the _ltmle_ package is designed to handle longitudinal data, with measurements data recorded for each subject at multiple timepoints. More information on the use of the _ltmle_ package in these settings can be found [here](https://cran.r-project.org/web/packages/ltmle/vignettes/ltmle-intro.html).

## tmle3
The _tmle3_ package uses the _sl3_ package rather than the _SuperLearner_ library, and is implemented to be more generalizable than the _tmle_ package. It is important to note that this package is still in the early stages of development.

```{r tmle3_pkg}
# create the node list, specifying outcome, exposure, and covariates
node_list <- list(
  W = colnames(ObsData)[-which(names(ObsData) %in% c("Y", "A", "Y_transf"))],
  A = "A", 
  Y = "Y"
)

# create a spec object, denoting treatment variable and control level
rhc_spec <- tmle_ATE(
  treatment_level = "A",
  control_level = 1
)

# choose sl3 learners to be used (we will use same stack we created in sl3 section) and make an ensemble learner for the outcome and the exposure
# 
# sl_A <- make_learner(Lrnr_sl, learners = stack)
# sl_Y <- make_learner(Lrnr_sl, learners = stack)
# 
# learner_list <- list(A=sl_A, Y=sl_Y)

# choose base learners
require(ranger)
lrnr_mean <- make_learner(Lrnr_mean)
lrnr_rf <- make_learner(Lrnr_ranger)

# define metalearners appropriate to data types
ls_metalearner <- make_learner(Lrnr_nnls)
mn_metalearner <- make_learner(
  Lrnr_solnp, metalearner_linear_multinomial,
  loss_loglik_multinomial
)
sl_Y <- Lrnr_sl$new(
  learners = list(lrnr_mean, lrnr_rf),
  metalearner = ls_metalearner
)
sl_A <- Lrnr_sl$new(
  learners = list(lrnr_mean, lrnr_rf),
  metalearner = mn_metalearner
)
learner_list <- list(A = sl_A, Y = sl_Y)

processed <- process_missing(ObsData, node_list)
ObsData_p <- processed$data
node_list <- processed$node_list

# fit TMLE
# tmle3_fit <- tmle3(rhc_spec, subset(ObsData_p, select = -Y_transf), node_list, learner_list)

# TODO
# tmle3_fit <- tmle3(rhc_spec, ObsData_p, node_list, learner_list)
# print(tmle3_fit)

# TODO
tmle3_ci <- paste("(", NA, ", ", NA, ")", sep = "")

```


## AIPW
The _aipw_ package implements augmented inverse probability weighting. It has similar parameters as the _tmle_ package. It is typically used with the _SuperLearner_ library. 

```{r aipw_pkg, cache=TRUE}
set.seed(1444) 

# construct AIPW estimator
aipw <- AIPW$new(Y=ObsData$Y, 
                 A=ObsData$A, 
                 W=ObsData[colnames(ObsData)[-which(names(ObsData) == "Y")]], 
                 Q.SL.library = c("SL.mean", "SL.glm"), 
                 g.SL.library = c("SL.mean", "SL.glm"), 
                 k_split=3, 
                 verbose=FALSE)
# fit AIPW object
aipw$fit()

# calculate ATE
aipw$summary()
print(aipw$estimates)
aipw_est <- aipw$estimates$RD[["Estimate"]]

# 95% CI
aipw_ci <- paste(" (", round(aipw$estimates$RD["95% LCL"], 3), ", ", round(aipw$estimates$RD["95% UCL"], 3), ")", sep = "")

cat("ATE from aipw package: ", aipw_est, aipw_ci, sep = "")
```


## RHC results
```{r summary table, cache=TRUE}
methods <- c("sl3", "tmle", "ltmle", "tmle3", "aipw")
estimates <- round(c(sl3_est, tmle_est, ltmle_est$estimates[["tmle"]], NA, aipw_est), 3)

CIs <- c(sl3_ci, tmle_ci, ltmle_ci, tmle3_ci, aipw_ci)

results <- data.frame(methods, estimates, CIs)
results
```


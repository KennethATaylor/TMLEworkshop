
```{r setup04, include=FALSE}
require(knitr)
require(glmnet)
require(kableExtra)
require(dplyr)
require(xgboost)
require(SuperLearner)
require(sl3)
require(Rsolnp)
require(ltmle)
require(AIPW)
require(tmle3)
options(knitr.kable.NA = '')
cachex=TRUE
```

```{r dataload_01, cache=cachex, echo = TRUE}
# Read the data saved at the last chapter
ObsData <- readRDS(file = "data/rhcAnalytic.RDS")
dim(ObsData)
```

# Pre-packaged software

## sl3
The _sl3_ package implements two types of Super Learning: discrete Super Learning, in which the best prediction algorithm (based on cross-validation) from a specified library is returned, and ensemble Super Learning, in which the best linear combination of the specified algorithms is returned (@coyle2021sl3).

The first step is to create a sl3 task which keeps track of the roles of the variables in our problem (@coyle2021tlverse). 

```{r sl3_01}
# create sl3 task, specifying outcome and covariates 
rhc_task <- make_sl3_Task(
  data = ObsData, 
  covariates = colnames(ObsData)[-which(names(ObsData) == "Y")],
  outcome = "Y"
)
print(rhc_task)
```

Next, we create our SuperLearner. To do this, we need to specify a selection of machine learning algorithms we want to include as candidates, as well as a metalearner that the SuperLearner will use to combine or choose from the machine learning algorithms provided (@coyle2021tlverse). 
```{r sl3_02}
# see what algorithms are available for a continuous outcome (similar can be done for a binary outcome)
sl3_list_learners("continuous")
```

The chosen candidate algorithms can be created and collected in a Stack.
```{r sl3_03, results='hide', message=FALSE, warning=FALSE}
# initialize candidate learners
lrn_glm <- make_learner(Lrnr_glm)
lrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1
xgb_50 <- Lrnr_xgboost$new(nrounds = 50)

# collect learners in stack
stack <- make_learner(
  Stack, lrn_glm, lrn_lasso, xgb_50
)
```

The stack is then given to the SuperLearner.
```{r sl3_04, results='hide', message=FALSE, warning=FALSE}
# to make an ensemble SuperLearner
sl_meta <- Lrnr_nnls$new()
sl <- Lrnr_sl$new(
  learners = stack,
  metalearner = sl_meta)

# or a discrete SuperLearner
sl_disc_meta <- Lrnr_cv_selector$new()
sl_disc <- Lrnr_sl$new(
  learners = stack, 
  metalearner = sl_disc_meta
)
```

The SuperLearner is then trained on the sl3 task we created at the start and then it can be used to make predictions.
```{r sl3_05, results='hide', message=FALSE, warning=FALSE}
set.seed(1444)

# train SL
sl_fit <- sl$train(rhc_task)
# or for discrete SL
# sl_fit <- sl_disc$train(rhc_task)

# make predictions
sl3_data <- ObsData
sl3_data$sl_preds <- sl_fit$predict()

sl3_est <- mean(sl3_data$sl_preds[sl3_data$A == 1]) - mean(sl3_data$sl_preds[sl3_data$A == 0])
```

```{r, echo=FALSE}
print(sl3_est)

# TODO
sl3_ci <- paste("(", NA, ", ", NA, ")", sep = "")
```
Notes about the _sl3_ package: 

* fairly easy to implement & understand structure
* large selection of candidate algorithms provided
* unsure why result is so different
* very different structure from _SuperLearner_ library, but very customizable
* could use more explanations of when to use what metalearner and what exactly the structure of the metalearner construction means 

Most helpful resources: 

* [tlverse sl3 page](https://tlverse.org/sl3/)
* [sl3 GitHub repository](https://github.com/tlverse/sl3/)
* [tlverse handbook chapter 6](https://tlverse.org/tlverse-handbook/tmle3.html)
* Vignettes in R

## tmle
The _tmle_ package can handle both binary and continuous outcomes, and uses the _SuperLearner_ package to construct both models just like we did in the steps above. The default SuperLearner library for estimating the outcome includes generalized linear models (GLMs), GLM with elastic net regularization, and Bayesian additive regression trees. The default library for estimating the propensity scores also includes GLMs and Bayesian additive regression trees (though specified slightly differently), and replaces the GLM with regularization with generalized additive models (GAMs) (@tmlePkgDocs). More methods can be added by specifying lists of models in the _Q.SL.library_ (for the outcome model) and _g.SL.library_ (for the propensity score model) arguments. Note also that the outcome $Y$ is required to be within the range of $[0,1]$ for this method as well, so we need to pass in the transformed data, then transform back the estimate.

```{r tmle_pkg, cache=TRUE, results='hide', message=FALSE, warning=FALSE}
set.seed(1444) 

# transform the outcome to fall within the range [0,1]
a <- min(na.omit(ObsData$Y))
b <- max(na.omit(ObsData$Y))
ObsData$Y_transf <- (ObsData$Y-a)/(b-a)

# run tmle from the tmle package 
W <- dplyr::select(ObsData, !c(Y_transf, Y, A))
SL.library = c("SL.glm", "SL.glmnet", "SL.xgboost")
tmle <- tmle::tmle(Y = ObsData$Y_transf, A = ObsData$A, W = W, family = "gaussian", Q.SL.library = SL.library, g.SL.library = SL.library)
tmle_est_tr <- tmle$estimates$ATE$psi
# transform back the ATE estimate
tmle_est <- (b-a)*tmle_est_tr

tmle_ci <- paste("(", round((b-a)*tmle$estimates$ATE$CI[1], 3), ", ", round((b-a)*tmle$estimates$ATE$CI[2], 3), ")", sep = "")
```

```{r, echo=FALSE}
cat("ATE from tmle package: ", tmle_est, tmle_ci, sep = "")
```
Notes about the _tmle_ package: 

* does not scale the outcome for you
* can give some confusing error messages when dealing with variable types it is not expecting
* practically all steps are nicely packed up in one function, very easy to use but need to dig a little to truly understand what it does
* at first was a bit difficult to figure out how to use with a continous outcome and log-likelihood loss function as the difference between several parameters relating to variable type and loss function was unclear

Most helpful resources: 

* [CRAN docs](https://cran.r-project.org/web/packages/tmle/tmle.pdf)
* [tmle package paper](https://www.jstatsoft.org/article/view/v051i13)
* Vignettes in R

## ltmle
Similarly to the _tmle_ package, the _ltmle_ package gives the direct TMLE result with the call of one function. 

```{r ltmle_pkg, cache=TRUE, message=FALSE, warning=FALSE}

# ltmle(data=data, Anodes=' A' , Ynodes=' Y' , abar=list(1,0),
# SL.library=SL.library, estimate.time=F)


# exclude Y_transf since ltmle scales automatically
ltmle_data <- dplyr::select(ObsData, !Y_transf) 

# run ltmle
ltmle_est <- ltmle(ltmle_data, Anodes = "A", Ynodes = "Y", abar = 1, SL.library = SL.library)

# print result & confidence intervals
ltmle_ci <- paste("(", round(summary(ltmle_est)[["treatment"]][["CI"]][,"2.5%"], 3), ", ", round(summary(ltmle_est)[["treatment"]][["CI"]][,"97.5%"], 3), ")", sep = "")
cat("ATE from ltmle package: ", ltmle_est$estimates[["tmle"]], ltmle_ci, sep = "")
```
The main difference between the _tmle_ and the _ltmle_ package is that the _ltmle_ package is designed to handle longitudinal data, with measurements data recorded for each subject at multiple timepoints. More information on the use of the _ltmle_ package in these settings can be found [here](https://cran.r-project.org/web/packages/ltmle/vignettes/ltmle-intro.html).

## AIPW
The _aipw_ package implements augmented inverse probability weighting. It has similar parameters as the _tmle_ package. It is typically used with the _SuperLearner_ library. 

```{r aipw_pkg, cache=TRUE, results='hide', message=FALSE, warning=FALSE}
set.seed(1444) 
# construct AIPW estimator
aipw <- AIPW$new(Y=ObsData$Y, 
                 A=ObsData$A, 
                 W=ObsData[colnames(ObsData)[-which(names(ObsData) == "Y")]], 
                 Q.SL.library = c("SL.mean", "SL.glm"), 
                 g.SL.library = c("SL.mean", "SL.glm"), 
                 k_split=3, 
                 verbose=FALSE)
# fit AIPW object
aipw$fit()
# calculate ATE
aipw$summary()
print(aipw$estimates)
aipw_est <- aipw$estimates$RD[["Estimate"]]

# 95% CI
aipw_ci <- paste(" (", round(aipw$estimates$RD["95% LCL"], 3), ", ", round(aipw$estimates$RD["95% UCL"], 3), ")", sep = "")
```

```{r, echo=FALSE}
cat("ATE from aipw package: ", aipw_est, aipw_ci, sep = "")
```

## RHC results
```{r summary table, cache=TRUE, echo=FALSE}
methods <- c("sl3", "tmle", "ltmle", "aipw")
estimates <- round(c(sl3_est, tmle_est, ltmle_est$estimates[["tmle"]], aipw_est), 3)

CIs <- c(sl3_ci, tmle_ci, ltmle_ci, aipw_ci)

results <- data.frame(methods, estimates, CIs)
results
```

## Other packages
Other packages that may be useful: 

| Package | Resources | Notes |
|---|---|---|
| tmle3 | [GitHub](https://github.com/tlverse/tmle3), [framework overview](https://tlverse.org/tmle3/articles/framework.html), [tlverse handbook](https://tlverse.org/tlverse-handbook/tmle3.html) | tmle3 is still under development | 
| aipw | [GitHub](https://github.com/yqzhong7/AIPW), [CRAN vignette](https://cran.r-project.org/web/packages/AIPW/vignettes/AIPW.html) | |


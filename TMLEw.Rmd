--- 
title: "R Guide for TMLE in Medical Research"
author: ""
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::html_document2:
    includes:
      in_header: header.html
  bookdown::gitbook:
    includes:
      in_header: header.html
  bookdown::pdf_book:
    includes:
      in_header: header.html
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: ehsanx/intro2R
description: "Intro to R."
---

# Preface {-}

Placeholder


## Background  {-}
## Goal  {-}
## Philosophy  {-}
## Pre-requisites {-}
## Version history {-}
## Contributor list {-}
## License {-}

<!--chapter:end:index.Rmd-->


# RHC data description

Placeholder


## Data download
## Analytic data
## Notations
## Variables
## Table 1 stratified by RHC exposure
## Basic regression analysis
### Crude analysis
### Adjusted analysis
### Regression diagnostics
## Comparison with literature
### PSM
### PSM diagnostics
### PSM results

<!--chapter:end:1RHC.Rmd-->


# G-computation

Placeholder


## Closer look at the data
### View data from 6 participants
### Restructure the data to estimate treatment effect
### Treat the problem as a missing value problem
### Impute better value?
## Use Regression for predicting outcome
### Predict outcome for treated
### Look at the predicted outcome data for treated
### Predict outcome for untreated
### Look at the predicted outcome data for untreated
### Look at the predicted outcome data for all!
## Parametric G-computation
### Steps
### Treatment effect estimate
## Estimating the confidence intervals

<!--chapter:end:2gcomp.Rmd-->


# G-computation using ML

Placeholder


## G-comp using Regression tree
### A tree based algorithm
### Cross-validation
### Extract outcome prediction as if everyone is treated
### Extract outcome prediction as if everyone is untreated
### Treatment effect estimate
## G-comp using regularized methods
### A regularized model
### Extract outcome prediction as if everyone is treated
### Extract outcome prediction as if everyone is untreated
### Treatment effect estimate
## G-comp using SuperLearner
### Steps  
### Extract outcome prediction as if everyone is treated
### Extract outcome prediction as if everyone is untreated
### Treatment effect estimate
### Additional details for SL

<!--chapter:end:2gcomp2.Rmd-->


# IPTW

Placeholder


## IPTW steps 
## Step 1: exposure modelling
## Step 2: Convert PS to IPW 
## Step 3: Balance checking
## Step 4: outcome modelling 

<!--chapter:end:3ipw.Rmd-->


# IPTW using ML

Placeholder


## IPTW Steps from SL
## Step 1: exposure modelling
## Step 2: Convert PS to IPW 
## Step 3: Balance checking
## Step 4: outcome modelling 

<!--chapter:end:3ipw2.Rmd-->


# TMLE

Placeholder


## Doubly robust estimators
## TMLE
## TMLE Steps
## Step 1: Transformation of Y
## Step 2: Initial G-comp estimate
### Get predictions under both treatments $A = 0$ and $1$
### Get initial treatment effect estimate
## Step 3: PS model
## Step 4: Estimate $H$ 
## Step 5: Estimate $\epsilon$
### $\hat\epsilon$ = $\hat\epsilon_0$ and $\hat\epsilon_1$ 
### Only 1 $\hat\epsilon$
## Step 6: Update
### $\hat\epsilon$ = $\hat\epsilon_0$ and $\hat\epsilon_1$ 
### Only 1 $\hat\epsilon$
## Step 7: Effect estimate
### $\hat\epsilon$ = $\hat\epsilon_0$ and $\hat\epsilon_1$ 
### Only 1 $\hat\epsilon$
## Step 8: Rescale effect estimate
### $\hat\epsilon$ = $\hat\epsilon_0$ and $\hat\epsilon_1$ 
### Only 1 $\hat\epsilon$
## Step 9: Confidence interval estimation
### $\hat\epsilon$ = $\hat\epsilon_0$ and $\hat\epsilon_1$ 
### Only 1 $\hat\epsilon$

<!--chapter:end:4tmle.Rmd-->


```{r setup04, include=FALSE}
require(knitr)
require(glmnet)
require(kableExtra)
require(dplyr)
require(xgboost)
require(SuperLearner)
require(sl3)
require(Rsolnp)
require(ltmle)
require(AIPW)
require(tmle3)
require(sl3)
options(knitr.kable.NA = '')
cachex=TRUE
```

```{r dataload_01, cache=cachex, echo = TRUE}
# Read the data saved at the last chapter
ObsData <- readRDS(file = "data/rhcAnalytic.RDS")
dim(ObsData)
```

# Pre-packaged software

## sl3

```{r}
# install sl3 if not done so
# remotes::install_github("tlverse/sl3")
```


The _sl3_ package implements two types of Super Learning: 

- discrete Super Learning, 
  - in which the best prediction algorithm (based on cross-validation) from a specified library is returned, and 
- ensemble Super Learning, 
  - in which the best linear combination of the specified algorithms is returned (@coyle2021sl3).

The first step is to create a sl3 task which keeps track of the roles of the variables in our problem (@coyle2021tlverse). 

```{r sl3_01}
require(sl3)
# create sl3 task, specifying outcome and covariates 
rhc_task <- make_sl3_Task(
  data = ObsData, 
  covariates = colnames(ObsData)[-which(
    names(ObsData) == "Y")],
  outcome = "Y"
)
print(rhc_task)
```

Next, we create our SuperLearner. To do this, 

- we need to specify a selection of machine learning algorithms we want to include as candidates, as well as 
- a metalearner that the SuperLearner will use to combine or choose from the machine learning algorithms provided (@coyle2021tlverse). 

```{r sl3_02}
# see what algorithms are available for a continuous outcome 
# (similar can be done for a binary outcome)
sl3_list_learners("continuous")
```

The chosen candidate algorithms can be created and collected in a Stack.

```{r sl3_03, results='hide', message=FALSE, warning=FALSE}
# initialize candidate learners
lrn_glm <- make_learner(Lrnr_glm)
lrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1
xgb_50 <- Lrnr_xgboost$new(nrounds = 50)

# collect learners in stack
stack <- make_learner(
  Stack, lrn_glm, lrn_lasso, xgb_50
)
```

The stack is then given to the SuperLearner.
```{r sl3_04, results='hide', message=FALSE, warning=FALSE}
# to make an ensemble SuperLearner
sl_meta <- Lrnr_nnls$new()
sl <- Lrnr_sl$new(
  learners = stack,
  metalearner = sl_meta)

# or a discrete SuperLearner
sl_disc_meta <- Lrnr_cv_selector$new()
sl_disc <- Lrnr_sl$new(
  learners = stack, 
  metalearner = sl_disc_meta
)
```

The SuperLearner is then trained on the sl3 task we created at the start and then it can be used to make predictions.

```{r sl3_05, results='hide', message=FALSE, warning=FALSE}
set.seed(1444)

# train SL
sl_fit <- sl$train(rhc_task)
# or for discrete SL
# sl_fit <- sl_disc$train(rhc_task)

# make predictions
sl3_data <- ObsData
sl3_data$sl_preds <- sl_fit$predict()

sl3_est <- mean(sl3_data$sl_preds[sl3_data$A == 1]) - mean(sl3_data$sl_preds[sl3_data$A == 0])
```

```{r, echo=FALSE}
print(sl3_est)

# TODO
sl3_ci <- paste("(", NA, ", ", NA, ")", sep = "")
```

Notes about the _sl3_ package: 

* fairly easy to implement & understand structure
* large selection of candidate algorithms provided
* unsure why result is so different
* very different structure from _SuperLearner_ library, but very customizable
* could use more explanations of when to use what metalearner and what exactly the structure of the metalearner construction means 

Most helpful resources: 

* [tlverse sl3 page](https://tlverse.org/sl3/)
* [sl3 GitHub repository](https://github.com/tlverse/sl3/)
* [tlverse handbook chapter 6](https://tlverse.org/tlverse-handbook/tmle3.html)
* Vignettes in R

## tmle

- The _tmle_ package can handle 
  - both binary and 
  - continuous outcomes, and 
  - uses the _SuperLearner_ package to construct both models just like we did in the steps above.
- The default SuperLearner library for estimating the outcome includes 
  - generalized linear models (GLMs), 
  - GLM with elastic net regularization, and 
  - Bayesian additive regression trees. 
- The default library for estimating the propensity scores also includes 
  - GLMs and 
  - Bayesian additive regression trees (though specified slightly differently), and replaces the GLM with regularization with generalized additive models (GAMs) (@tmlePkgDocs). 
  - More methods can be added by specifying lists of models in the _Q.SL.library_ (for the outcome model) and _g.SL.library_ (for the propensity score model) arguments. 
- Note also that the outcome $Y$ is required to be within the range of $[0,1]$ for this method as well, so we need to pass in the transformed data, then transform back the estimate.

```{r tmle_pkg, cache=TRUE, results='hide', message=FALSE, warning=FALSE}
set.seed(1444) 

# transform the outcome to fall within the range [0,1]
min.Y <- min(ObsData$Y)
max.Y <- max(ObsData$Y)
ObsData$Y_transf <- (ObsData$Y-min.Y)/(max.Y-min.Y)

# run tmle from the tmle package 
ObsData.noYA <- dplyr::select(ObsData, 
                              !c(Y_transf, Y, A))
SL.library = c("SL.glm", 
               "SL.glmnet", 
               "SL.xgboost")
tmle <- tmle::tmle(Y = ObsData$Y_transf, 
                   A = ObsData$A, 
                   W = ObsData.noYA, 
                   family = "gaussian", 
                   Q.SL.library = SL.library, 
                   g.SL.library = SL.library)
tmle_est_tr <- tmle$estimates$ATE$psi
# transform back the ATE estimate
tmle_est <- (max.Y-min.Y)*tmle_est_tr

tmle_ci <- paste("(", 
                 round((max.Y-min.Y)*tmle$estimates$ATE$CI[1], 3), ", ", round((b-a)*tmle$estimates$ATE$CI[2], 3), ")", sep = "")
```

```{r, echo=FALSE}
cat("ATE from tmle package: ", tmle_est, tmle_ci, sep = "")
```

Notes about the _tmle_ package: 

* does not scale the outcome for you
* can give some error messages when dealing with variable types it is not expecting
* practically all steps are nicely packed up in one function, very easy to use but need to dig a little to truly understand what it does
* at first was not straightforward to figure out how to use with a continuous outcome and log-likelihood loss function as the difference between several parameters relating to variable type and loss function was unclear

Most helpful resources: 

* [CRAN docs](https://cran.r-project.org/web/packages/tmle/tmle.pdf)
* [tmle package paper](https://www.jstatsoft.org/article/view/v051i13)
* Vignettes in R

## ltmle

Similarly to the _tmle_ package, the _ltmle_ package gives the direct TMLE result with the call of one function. 

```{r ltmle_pkg, cache=TRUE, message=FALSE, warning=FALSE}

# ltmle(data=data, Anodes=' A' , Ynodes=' Y' , abar=list(1,0),
# SL.library=SL.library, estimate.time=F)


# exclude Y_transf since ltmle scales automatically
ltmle_data <- dplyr::select(ObsData, !Y_transf) 

# run ltmle
ltmle_est <- ltmle(ltmle_data, 
                   Anodes = "A", 
                   Ynodes = "Y", 
                   abar = 1, 
                   SL.library = SL.library)

# print result & confidence intervals
ltmle_ci <- paste("(", 
                  round(summary(ltmle_est)[["treatment"]][["CI"]][,"2.5%"], 3), ", ", round(summary(ltmle_est)[["treatment"]][["CI"]][,"97.5%"], 3), ")", sep = "")
cat("ATE from ltmle package: ", 
    ltmle_est$estimates[["tmle"]], ltmle_ci, sep = "")
```

- The main difference between the _tmle_ and the _ltmle_ package is that the _ltmle_ package is designed to handle longitudinal data, with measurements data recorded for each subject at multiple timepoints. 
- More information on the use of the _ltmle_ package in these settings can be found [here](https://cran.r-project.org/web/packages/ltmle/vignettes/ltmle-intro.html).

## AIPW

- The _aipw_ package implements augmented inverse probability weighting (another type of DR method). 
- It has similar parameters as the _tmle_ package. 
- It is typically used with the _SuperLearner_ library. 

```{r aipw_pkg, cache=TRUE, results='hide', message=FALSE, warning=FALSE}
set.seed(1444) 
# construct AIPW estimator
aipw <- AIPW$new(Y=ObsData$Y, 
                 A=ObsData$A, 
                 W=ObsData[colnames(ObsData)[-which(names(ObsData) == "Y")]], 
                 Q.SL.library = c("SL.mean", "SL.glm"), 
                 g.SL.library = c("SL.mean", "SL.glm"), 
                 k_split=3, 
                 verbose=FALSE)
# fit AIPW object
aipw$fit()
# calculate ATE
aipw$summary()
print(aipw$estimates)
aipw_est <- aipw$estimates$RD[["Estimate"]]

# 95% CI
aipw_ci <- paste(" (", 
                 round(aipw$estimates$RD["95% LCL"], 3), ", ", 
                 round(aipw$estimates$RD["95% UCL"], 3), ")", sep = "")
```

```{r, echo=FALSE}
cat("ATE from aipw package: ", aipw_est, aipw_ci, sep = "")
```

## RHC results
```{r summary table, cache=TRUE, echo=FALSE}
methods <- c("sl3", "tmle", "ltmle", "aipw")
estimates <- round(c(sl3_est, tmle_est, 
                     ltmle_est$estimates[["tmle"]], 
                     aipw_est), 3)

CIs <- c(sl3_ci, tmle_ci, ltmle_ci, aipw_ci)

results <- data.frame(methods, estimates, CIs)
results
```

## Other packages

Other packages that may be useful: 

| Package | Resources | Notes |
|---|---|---|
| tmle3 | [GitHub](https://github.com/tlverse/tmle3), [framework overview](https://tlverse.org/tmle3/articles/framework.html), [tlverse handbook](https://tlverse.org/tlverse-handbook/tmle3.html) | tmle3 is still under development | 
| aipw | [GitHub](https://github.com/yqzhong7/AIPW), [CRAN vignette](https://cran.r-project.org/web/packages/AIPW/vignettes/AIPW.html) | |


<!--chapter:end:5software.Rmd-->


# Final Words

Placeholder


## Select variables judiciously
## Why SL and TMLE
### Prediction goal
### Causal inference
### Identifiability assumptions
## Further reading
### Key articles
### Additional readings
### Workshops
### Recorded webinars
#### Introductory materials
#### More theory talks
#### More applied talks

<!--chapter:end:6final.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`
<div id="refs"></div>


<!--chapter:end:7references.Rmd-->


# Preface {-}

Placeholder


## Background  {-}
## Goal  {-}
## Philosophy  {-}
## Pre-requisites {-}
## Version history {-}
## Contributor list {-}
## License {-}

<!--chapter:end:index0.Rmd-->


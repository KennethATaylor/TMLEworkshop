[["pre-packaged-software.html", "Chapter 7 Pre-packaged software 7.1 sl3 7.2 tmle 7.3 ltmle 7.4 AIPW 7.5 RHC results 7.6 Other packages", " Chapter 7 Pre-packaged software 7.1 sl3 # install sl3 if not done so # remotes::install_github(&quot;tlverse/sl3&quot;) The sl3 package implements two types of Super Learning: discrete Super Learning, in which the best prediction algorithm (based on cross-validation) from a specified library is returned, and ensemble Super Learning, in which the best linear combination of the specified algorithms is returned (Coyle, Hejazi, Malenica, et al. (2021)). The first step is to create a sl3 task which keeps track of the roles of the variables in our problem (Coyle, Hejazi, Melencia, et al. (2021)). require(sl3) # create sl3 task, specifying outcome and covariates rhc_task &lt;- make_sl3_Task( data = ObsData, covariates = colnames(ObsData)[-which(names(ObsData) == &quot;Y&quot;)], outcome = &quot;Y&quot; ) rhc_task ## A sl3 Task with 5735 obs and these nodes: ## $covariates ## [1] &quot;Disease.category&quot; &quot;Cancer&quot; &quot;Cardiovascular&quot; ## [4] &quot;Congestive.HF&quot; &quot;Dementia&quot; &quot;Psychiatric&quot; ## [7] &quot;Pulmonary&quot; &quot;Renal&quot; &quot;Hepatic&quot; ## [10] &quot;GI.Bleed&quot; &quot;Tumor&quot; &quot;Immunosupperssion&quot; ## [13] &quot;Transfer.hx&quot; &quot;MI&quot; &quot;age&quot; ## [16] &quot;sex&quot; &quot;edu&quot; &quot;DASIndex&quot; ## [19] &quot;APACHE.score&quot; &quot;Glasgow.Coma.Score&quot; &quot;blood.pressure&quot; ## [22] &quot;WBC&quot; &quot;Heart.rate&quot; &quot;Respiratory.rate&quot; ## [25] &quot;Temperature&quot; &quot;PaO2vs.FIO2&quot; &quot;Albumin&quot; ## [28] &quot;Hematocrit&quot; &quot;Bilirubin&quot; &quot;Creatinine&quot; ## [31] &quot;Sodium&quot; &quot;Potassium&quot; &quot;PaCo2&quot; ## [34] &quot;PH&quot; &quot;Weight&quot; &quot;DNR.status&quot; ## [37] &quot;Medical.insurance&quot; &quot;Respiratory.Diag&quot; &quot;Cardiovascular.Diag&quot; ## [40] &quot;Neurological.Diag&quot; &quot;Gastrointestinal.Diag&quot; &quot;Renal.Diag&quot; ## [43] &quot;Metabolic.Diag&quot; &quot;Hematologic.Diag&quot; &quot;Sepsis.Diag&quot; ## [46] &quot;Trauma.Diag&quot; &quot;Orthopedic.Diag&quot; &quot;race&quot; ## [49] &quot;income&quot; &quot;A&quot; ## ## $outcome ## [1] &quot;Y&quot; ## ## $id ## NULL ## ## $weights ## NULL ## ## $offset ## NULL ## ## $time ## NULL Next, we create our SuperLearner. To do this, we need to specify a selection of machine learning algorithms we want to include as candidates, as well as a metalearner that the SuperLearner will use to combine or choose from the machine learning algorithms provided (Coyle, Hejazi, Melencia, et al. (2021)). # see what algorithms are available for a continuous outcome # (similar can be done for a binary outcome) sl3_list_learners(&quot;continuous&quot;) ## [1] &quot;Lrnr_arima&quot; &quot;Lrnr_bartMachine&quot; ## [3] &quot;Lrnr_bilstm&quot; &quot;Lrnr_bound&quot; ## [5] &quot;Lrnr_caret&quot; &quot;Lrnr_cv_selector&quot; ## [7] &quot;Lrnr_dbarts&quot; &quot;Lrnr_earth&quot; ## [9] &quot;Lrnr_expSmooth&quot; &quot;Lrnr_gam&quot; ## [11] &quot;Lrnr_gbm&quot; &quot;Lrnr_glm&quot; ## [13] &quot;Lrnr_glm_fast&quot; &quot;Lrnr_glmnet&quot; ## [15] &quot;Lrnr_grf&quot; &quot;Lrnr_gru_keras&quot; ## [17] &quot;Lrnr_gts&quot; &quot;Lrnr_h2o_glm&quot; ## [19] &quot;Lrnr_h2o_grid&quot; &quot;Lrnr_hal9001&quot; ## [21] &quot;Lrnr_HarmonicReg&quot; &quot;Lrnr_hts&quot; ## [23] &quot;Lrnr_lstm&quot; &quot;Lrnr_lstm_keras&quot; ## [25] &quot;Lrnr_mean&quot; &quot;Lrnr_multiple_ts&quot; ## [27] &quot;Lrnr_nnet&quot; &quot;Lrnr_nnls&quot; ## [29] &quot;Lrnr_optim&quot; &quot;Lrnr_pkg_SuperLearner&quot; ## [31] &quot;Lrnr_pkg_SuperLearner_method&quot; &quot;Lrnr_pkg_SuperLearner_screener&quot; ## [33] &quot;Lrnr_polspline&quot; &quot;Lrnr_randomForest&quot; ## [35] &quot;Lrnr_ranger&quot; &quot;Lrnr_rpart&quot; ## [37] &quot;Lrnr_rugarch&quot; &quot;Lrnr_screener_correlation&quot; ## [39] &quot;Lrnr_solnp&quot; &quot;Lrnr_stratified&quot; ## [41] &quot;Lrnr_svm&quot; &quot;Lrnr_tsDyn&quot; ## [43] &quot;Lrnr_xgboost&quot; The chosen candidate algorithms can be created and collected in a Stack. # initialize candidate learners lrn_glm &lt;- make_learner(Lrnr_glm) lrn_lasso &lt;- make_learner(Lrnr_glmnet) # alpha default is 1 xgb_5 &lt;- Lrnr_xgboost$new(nrounds = 5) # collect learners in stack stack &lt;- make_learner( Stack, lrn_glm, lrn_lasso, xgb_5 ) The stack is then given to the SuperLearner. # to make an ensemble SuperLearner sl_meta &lt;- Lrnr_nnls$new() sl &lt;- Lrnr_sl$new( learners = stack, metalearner = sl_meta) # or a discrete SuperLearner sl_disc_meta &lt;- Lrnr_cv_selector$new() sl_disc &lt;- Lrnr_sl$new( learners = stack, metalearner = sl_disc_meta ) The SuperLearner is then trained on the sl3 task we created at the start and then it can be used to make predictions. set.seed(1444) # train SL sl_fit &lt;- sl$train(rhc_task) # or for discrete SL # sl_fit &lt;- sl_disc$train(rhc_task) # make predictions sl3_data &lt;- ObsData sl3_data$sl_preds &lt;- sl_fit$predict() sl3_est &lt;- mean(sl3_data$sl_preds[sl3_data$A == 1]) - mean(sl3_data$sl_preds[sl3_data$A == 0]) saveRDS(sl_fit, file = &quot;data/sl3.RData&quot;) ## [1] 4.922165 Notes about the sl3 package: fairly easy to implement &amp; understand structure large selection of candidate algorithms provided unsure why result is so different very different structure from SuperLearner library, but very customizable could use more explanations of when to use what metalearner and what exactly the structure of the metalearner construction means Most helpful resources: tlverse sl3 page sl3 GitHub repository tlverse handbook chapter 6 Vignettes in R 7.2 tmle The tmle package can handle both binary and continuous outcomes, and uses the SuperLearner package to construct both models just like we did in the steps above. The default SuperLearner library for estimating the outcome includes (Gruber, Van Der Laan, and Kennedy 2020) generalized linear models (GLMs), GLM with lasso regularization, and gradient boosting. The default library for estimating the propensity scores also include the same It is certainly possible to use different set of learners More methods can be added by specifying lists of models in the Q.SL.library (for the outcome model) and g.SL.library (for the propensity score model) arguments. Note also that the outcome \\(Y\\) is required to be within the range of \\([0,1]\\) for this method as well, so we need to pass in the transformed data, then transform back the estimate. set.seed(1444) # transform the outcome to fall within the range [0,1] min.Y &lt;- min(ObsData$Y) max.Y &lt;- max(ObsData$Y) ObsData$Y_transf &lt;- (ObsData$Y-min.Y)/(max.Y-min.Y) # run tmle from the tmle package ObsData.noYA &lt;- dplyr::select(ObsData, !c(Y_transf, Y, A)) SL.library = c(&quot;SL.glm&quot;, &quot;SL.glmnet&quot;, &quot;SL.xgboost&quot;) tmle.fit &lt;- tmle::tmle(Y = ObsData$Y_transf, A = ObsData$A, W = ObsData.noYA, family = &quot;gaussian&quot;, V = 3, Q.SL.library = SL.library, g.SL.library = SL.library) tmle.fit summary(tmle.fit) tmle_est_tr &lt;- tmle.fit$estimates$ATE$psi # transform back the ATE estimate tmle_est &lt;- (max.Y-min.Y)*tmle_est_tr saveRDS(tmle.fit, file = &quot;data/tmle.RData&quot;) tmle_ci &lt;- paste(&quot;(&quot;, round((max.Y-min.Y)*tmle.fit$estimates$ATE$CI[1], 3), &quot;, &quot;, round((max.Y-min.Y)*tmle.fit$estimates$ATE$CI[2], 3), &quot;)&quot;, sep = &quot;&quot;) ## ATE from tmle package: 2.870557(1.767, 3.974) Notes about the tmle package: does not scale the outcome for you can give some error messages when dealing with variable types it is not expecting practically all steps are nicely packed up in one function, very easy to use but need to dig a little to truly understand what it does at first was not straightforward to figure out how to use with a continuous outcome and log-likelihood loss function as the difference between several parameters relating to variable type and loss function was unclear Most helpful resources: CRAN docs tmle package paper Vignettes in R 7.3 ltmle Similarly to the tmle package, the ltmle package gives the direct TMLE result with the call of one function. # exclude Y_transf since ltmle scales automatically ltmle_data &lt;- dplyr::select(ObsData, !Y_transf) # run ltmle ltmle_est &lt;- ltmle(ltmle_data, Anodes = &quot;A&quot;, Ynodes = &quot;Y&quot;, abar = list(1,0), SL.cvControl=list(V=3), SL.library = SL.library, estimate.time = FALSE) summary(ltmle_est) ## Estimator: tmle ## Call: ## ltmle(data = ltmle_data, Anodes = &quot;A&quot;, Ynodes = &quot;Y&quot;, abar = list(1, ## 0), SL.library = SL.library, SL.cvControl = list(V = 3), ## estimate.time = FALSE) ## ## Treatment Estimate: ## Parameter Estimate: 21.559 ## Estimated Std Err: 0.34157 ## p-value: &lt;2e-16 ## 95% Conf Interval: (20.89, 22.229) ## ## Control Estimate: ## Parameter Estimate: 21.559 ## Estimated Std Err: 0.34157 ## p-value: &lt;2e-16 ## 95% Conf Interval: (20.89, 22.229) ## ## Additive Treatment Effect: ## Parameter Estimate: 2.448e-14 ## Estimated Std Err: 4.1647e-16 ## p-value: &lt;2e-16 ## 95% Conf Interval: (2.3664e-14, 2.5297e-14) saveRDS(ltmle_est, file = &quot;data/ltmle.RData&quot;) The main difference between the tmle and the ltmle package is that the ltmle package is designed to handle longitudinal data, with measurements data recorded for each subject at multiple timepoints. More information on the use of the ltmle package in these settings can be found here. 7.4 AIPW The aipw package implements augmented inverse probability weighting (another type of DR method). It has similar parameters as the tmle package. It is typically used with the SuperLearner library. set.seed(1444) # construct AIPW estimator aipw &lt;- AIPW$new(Y=ObsData$Y, A=ObsData$A, W=ObsData[colnames(ObsData)[-which(names(ObsData) == &quot;Y&quot;)]], Q.SL.library = c(&quot;SL.glm&quot;, &quot;SL.glmnet&quot;, &quot;SL.xgboost&quot;), g.SL.library = c(c(&quot;SL.glm&quot;, &quot;SL.glmnet&quot;, &quot;SL.xgboost&quot;)), k_split=3, verbose=FALSE) # fit AIPW object aipw$fit() # calculate ATE aipw$summary() print(aipw$estimates) aipw_est &lt;- aipw$estimates$RD[[&quot;Estimate&quot;]] # 95% CI aipw_ci &lt;- paste(&quot; (&quot;, round(aipw$estimates$RD[&quot;95% LCL&quot;], 3), &quot;, &quot;, round(aipw$estimates$RD[&quot;95% UCL&quot;], 3), &quot;)&quot;, sep = &quot;&quot;) saveRDS(aipw, file = &quot;data/aipw.RData&quot;) ## ATE from aipw package: 4.261266e-14 (0, 0) 7.5 RHC results ## methods estimates CIs ## 1 sl3 4.922 (NA, NA) ## 2 tmle 2.871 (1.767, 3.974) ## 3 aipw 0.000 (0, 0) 7.6 Other packages Other packages that may be useful: Package Resources Notes tmle3 GitHub, framework overview, tlverse handbook tmle3 is still under development aipw GitHub, CRAN vignette References "]]

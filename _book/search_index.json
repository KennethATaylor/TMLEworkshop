[["tmle.html", "Chapter 6 TMLE 6.1 Doubly robust estimators 6.2 TMLE 6.3 TMLE Steps 6.4 Step 1: Transformation of Y 6.5 Step 2: Initial G-comp estimate 6.6 Step 3: PS model 6.7 Step 4: Estimate \\(H\\) 6.8 Step 5: Estimate \\(\\epsilon\\) 6.9 Step 6: Update 6.10 Step 7: Effect estimate 6.11 Step 8: Rescale effect estimate 6.12 Step 9: Confidence interval estimation", " Chapter 6 TMLE 6.1 Doubly robust estimators Now that we have covered outcome models (e.g., G-computation) and exposure models (e.g., propensity score models), let us talk about Doubly robust (DR) estimators. DR has several important properties: They use information from both the exposure and the outcome models. They provide a consistent estimator if either of the above mentioned models is correctly specified. consistent estimator means as the sample size increases, distribution of the estimates gets concentrated near the true parameter They provide an efficient estimator if both the exposure and the outcome model are correctly specified. efficient estimator means estimates approximates the true parameter in terms of a chosen loss function (e.g., could be RMSE). 6.2 TMLE Targeted Maximum Likelihood Estimation (TMLE) is a DR method, using an initial estimate from the outcome model (G-computation) the propensity score (exposure) model to improve. In addition to being DR, TMLE has several other desirable properties: It allows the use of data-adaptive algorithms like machine learning without sacrificing interpretability. ML is only used in intermediary steps to develop the estimator, so the optimization and interpretation of the estimator as a whole remains intact. The use of machine learning can help mitigate model misspecification. It has been shown to outperform other methods, particularly in sparse data settings. 6.3 TMLE Steps According to Luque-Fernandez et al. (2018), we need to the following steps (2-7) for obtaining point estimates when dealing with binary outcome. But as we are dealing with continuous outcome, we need an added transformation step at the beginning, and also at the end. Step 1 Transformation of continuous outcome variable Step 2 Predict from initial outcome modelling: G-computation Step 3 Predict from propensity score model Step 4 Estimate clever covariate \\(H\\) Step 5 Estimate fluctuation parameter \\(\\epsilon\\) Step 6 Update the initial outcome model prediction based on targeted adjustment of the initial predictions using the PS model Step 7 Find treatment effect estimate Step 8 Transform back the treatment effect estimate in the original outcome scale Step 9 Confidence interval estimation based on closed form formula We will go through the steps of TMLE one-by-one, using the RHC dataset presented in previous chapters. As a reminder, the exposure we are considering is RHC (right heart catheterization) and the outcome of interest is length of stay in the hospital. # Read the data saved at the last chapter ObsData &lt;- readRDS(file = &quot;data/rhcAnalytic.RDS&quot;) 6.4 Step 1: Transformation of Y In our example, the outcome is continuous. summary(ObsData$Y) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 7.00 14.00 21.56 25.00 394.00 plot(density(ObsData$Y), main = &quot;Observed Y&quot;) General recommendation is to transform continuous outcome to be within the range [0,1] (Gruber and Laan 2010). min.Y &lt;- min(ObsData$Y) max.Y &lt;- max(ObsData$Y) ObsData$Y.bounded &lt;- (ObsData$Y-min.Y)/(max.Y-min.Y) Check the range of our transformed outcome variable summary(ObsData$Y.bounded) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00000 0.01276 0.03061 0.04990 0.05867 1.00000 6.5 Step 2: Initial G-comp estimate We construct our outcome model, and make our initial predictions. For this step, we will use SuperLearner. This requires no apriori assumptions about the structure of our outcome model. library(SuperLearner) set.seed(123) ObsData.noY &lt;- dplyr::select(ObsData, !c(Y,Y.bounded)) Y.fit.sl &lt;- SuperLearner(Y=ObsData$Y.bounded, X=ObsData.noY, cvControl = list(V = 3), SL.library=c(&quot;SL.glm&quot;, &quot;SL.glmnet&quot;, &quot;SL.xgboost&quot;), method=&quot;method.NNLS&quot;, family=&quot;gaussian&quot;) ObsData$init.Pred &lt;- predict(Y.fit.sl, newdata = ObsData.noY, type = &quot;response&quot;)$pred summary(ObsData$init.Pred) ## V1 ## Min. :-0.06060 ## 1st Qu.: 0.03775 ## Median : 0.05019 ## Mean : 0.04990 ## 3rd Qu.: 0.06188 ## Max. : 0.19344 # alternatively, we could write # ObsData$init.Pred &lt;- Y.fit.sl$SL.predict We will use these initial prediction values later. \\(Q^0(A,L)\\) is often used to represent the predictions from initial G-comp model. 6.5.1 Get predictions under both treatments \\(A = 0\\) and \\(1\\) We could estimate the treatment effect from this initial model. We will need the \\(Q^0(A=1,L)\\) and \\(Q^0(A=0,L)\\) predictions later. \\(Q^0(A=1,L)\\) predictions: ObsData.noY$A &lt;- 1 ObsData$Pred.Y1 &lt;- predict(Y.fit.sl, newdata = ObsData.noY, type = &quot;response&quot;)$pred summary(ObsData$Pred.Y1) ## V1 ## Min. :-0.06060 ## 1st Qu.: 0.04282 ## Median : 0.05488 ## Mean : 0.05428 ## 3rd Qu.: 0.06537 ## Max. : 0.19344 \\(Q^0(A=0,L)\\) predictions: 6.5.2 Get initial treatment effect estimate ObsData.noY$A &lt;- 0 ObsData$Pred.Y0 &lt;- predict(Y.fit.sl, newdata = ObsData.noY, type = &quot;response&quot;)$pred summary(ObsData$Pred.Y0) ## V1 ## Min. :-0.06724 ## 1st Qu.: 0.03598 ## Median : 0.04780 ## Mean : 0.04722 ## 3rd Qu.: 0.05821 ## Max. : 0.18408 ObsData$Pred.TE &lt;- ObsData$Pred.Y1 - ObsData$Pred.Y0 summary(ObsData$Pred.TE) ## V1 ## Min. :0.003033 ## 1st Qu.:0.006598 ## Median :0.006993 ## Mean :0.007061 ## 3rd Qu.:0.007468 ## Max. :0.012404 6.6 Step 3: PS model At this point, we have our initial estimate and now want to perform our targeted improvement. library(SuperLearner) set.seed(124) ObsData.noYA &lt;- dplyr::select(ObsData, !c(Y,Y.bounded, A,init.Pred, Pred.Y1,Pred.Y0, Pred.TE)) PS.fit.SL &lt;- SuperLearner(Y=ObsData$A, X=ObsData.noYA, cvControl = list(V = 3), SL.library=c(&quot;SL.glm&quot;, &quot;SL.glmnet&quot;, &quot;SL.xgboost&quot;), method=&quot;method.NNLS&quot;, family=&quot;binomial&quot;) all.pred &lt;- predict(PS.fit.SL, type = &quot;response&quot;) ObsData$PS.SL &lt;- all.pred$pred These propensity score predictions (PS.SL) are represented as \\(g(A_i=1|L_i)\\). We can estimate \\(g(A_i=0|L_i)\\) as \\(1 - g(A_i=1|L_i)\\) or 1 - PS.SL. summary(ObsData$PS.SL) ## V1 ## Min. :0.00269 ## 1st Qu.:0.14870 ## Median :0.34699 ## Mean :0.38083 ## 3rd Qu.:0.59316 ## Max. :0.97250 tapply(ObsData$PS.SL, ObsData$A, summary) ## $`0` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00269 0.08870 0.18953 0.22227 0.32987 0.78919 ## ## $`1` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.08083 0.51406 0.65538 0.63865 0.77110 0.97250 plot(density(ObsData$PS.SL[ObsData$A==0]), col = &quot;red&quot;, main = &quot;&quot;) lines(density(ObsData$PS.SL[ObsData$A==1]), col = &quot;blue&quot;, lty = 2) legend(&quot;topright&quot;, c(&quot;No RHC&quot;,&quot;RHC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty=1:2) 6.7 Step 4: Estimate \\(H\\) Clever covariate \\(H(A_i, L_i) = \\frac{I(A_i=1)}{g(A_i=1|L_i)} - \\frac{I(A_i=0)}{g(A_i=0|L_i)}\\) (Luque-Fernandez et al. 2018) ObsData$H.A1L &lt;- (ObsData$A) / ObsData$PS.SL ObsData$H.A0L &lt;- (1-ObsData$A) / (1- ObsData$PS.SL) ObsData$H.AL &lt;- ObsData$H.A1L - ObsData$H.A0L summary(ObsData$H.AL) ## V1 ## Min. :-4.7435 ## 1st Qu.:-1.3075 ## Median :-1.0767 ## Mean :-0.1826 ## 3rd Qu.: 1.3778 ## Max. :12.3720 tapply(ObsData$H.AL, ObsData$A, summary) ## $`0` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -4.744 -1.492 -1.234 -1.359 -1.097 -1.003 ## ## $`1` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.028 1.297 1.526 1.730 1.945 12.372 t(apply(cbind(-ObsData$H.A0L,ObsData$H.A1L), 2, summary)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## [1,] -4.743533 -1.307494 -1.076741 -0.8414464 0.000000 0.00000 ## [2,] 0.000000 0.000000 0.000000 0.6588811 1.377787 12.37196 Aggregated or individual clever covariate components show slight difference in their summaries. 6.8 Step 5: Estimate \\(\\epsilon\\) Fluctuation parameter \\(\\epsilon\\), representing how large of an adjustment we will make to the initial estimate. The fluctuation parameter \\(\\hat\\epsilon\\) could be a scalar or a vector with 2 components \\(\\hat\\epsilon_0\\) and \\(\\hat\\epsilon_1\\). It is estimated through MLE, using a model with an offset based on the initial estimate, and clever covariates as independent variables (Gruber and Van Der Laan 2009): \\(E(Y=1|A,L)(\\epsilon) = \\frac{1}{1+\\exp(-\\log\\frac{\\bar Q^0(A,L)}{(1-\\bar Q^0(A,L))}-\\epsilon \\times H(A,L))}\\) 6.8.1 \\(\\hat\\epsilon\\) = \\(\\hat\\epsilon_0\\) and \\(\\hat\\epsilon_1\\) This is more close to how how tmle package has implement clever covariates eps_mod &lt;- glm(Y.bounded ~ -1 + H.A1L + H.A0L + offset(qlogis(init.Pred)), family = &quot;binomial&quot;, data = ObsData) epsilon &lt;- coef(eps_mod) epsilon[&quot;H.A1L&quot;] ## H.A1L ## 0.00573477 epsilon[&quot;H.A0L&quot;] ## H.A0L ## 0.003333366 Note that, if init.Pred includes -ve values, NaNs would be produced after applying qlogis(). 6.8.2 Only 1 \\(\\hat\\epsilon\\) For demonstration purposes eps_mod1 &lt;- glm(Y.bounded ~ -1 + H.AL + offset(qlogis(init.Pred)), family = &quot;binomial&quot;, data = ObsData) epsilon1 &lt;- coef(eps_mod1) epsilon1 ## H.AL ## 0.001703316 Alternative could be to use H.AL as weights (not shown here). 6.9 Step 6: Update 6.9.1 \\(\\hat\\epsilon\\) = \\(\\hat\\epsilon_0\\) and \\(\\hat\\epsilon_1\\) We can use epsilon[\"H.A1L\"] and epsilon[\"H.A0L\"] to update ObsData$Pred.Y1.update &lt;- plogis(qlogis(ObsData$Pred.Y1) + epsilon[&quot;H.A1L&quot;]*ObsData$H.A1L) ObsData$Pred.Y0.update &lt;- plogis(qlogis(ObsData$Pred.Y0) + epsilon[&quot;H.A0L&quot;]*ObsData$H.A0L) summary(ObsData$Pred.Y1.update) ## V1 ## Min. :0.001473 ## 1st Qu.:0.043052 ## Median :0.055075 ## Mean :0.054543 ## 3rd Qu.:0.065632 ## Max. :0.194681 ## NA&#39;s :5 summary(ObsData$Pred.Y0.update) ## V1 ## Min. :0.000268 ## 1st Qu.:0.036216 ## Median :0.047958 ## Mean :0.047463 ## 3rd Qu.:0.058348 ## Max. :0.184080 ## NA&#39;s :12 6.9.2 Only 1 \\(\\hat\\epsilon\\) Alternatively, we could use epsilon to from H.AL to update ObsData$Pred.Y1.update1 &lt;- plogis(qlogis(ObsData$Pred.Y1) + epsilon1*ObsData$H.AL) ObsData$Pred.Y0.update1 &lt;- plogis(qlogis(ObsData$Pred.Y0) + epsilon1*ObsData$H.AL) summary(ObsData$Pred.Y1.update1) ## V1 ## Min. :0.00147 ## 1st Qu.:0.04292 ## Median :0.05491 ## Mean :0.05433 ## 3rd Qu.:0.06540 ## Max. :0.19381 ## NA&#39;s :5 summary(ObsData$Pred.Y0.update1) ## V1 ## Min. :0.000266 ## 1st Qu.:0.036073 ## Median :0.047785 ## Mean :0.047330 ## 3rd Qu.:0.058238 ## Max. :0.184433 ## NA&#39;s :12 Note that, if Pred.Y1 and Pred.Y0 include -ve values, NaNs would be produced after applying qlogis(). 6.10 Step 7: Effect estimate Now that the updated predictions of our outcome models are calculated, we can calculate the ATE. 6.10.1 \\(\\hat\\epsilon\\) = \\(\\hat\\epsilon_0\\) and \\(\\hat\\epsilon_1\\) ATE.TMLE.bounded.vector &lt;- ObsData$Pred.Y1.update - ObsData$Pred.Y0.update summary(ATE.TMLE.bounded.vector) ## V1 ## Min. :0.002861 ## 1st Qu.:0.006585 ## Median :0.007051 ## Mean :0.007141 ## 3rd Qu.:0.007626 ## Max. :0.013738 ## NA&#39;s :12 ATE.TMLE.bounded &lt;- mean(ATE.TMLE.bounded.vector, na.rm = TRUE) ATE.TMLE.bounded ## [1] 0.007140618 6.10.2 Only 1 \\(\\hat\\epsilon\\) Alternatively, using H.AL: ATE.TMLE.bounded.vector1 &lt;- ObsData$Pred.Y1.update1 - ObsData$Pred.Y0.update1 summary(ATE.TMLE.bounded.vector1) ## V1 ## Min. :0.003029 ## 1st Qu.:0.006599 ## Median :0.006994 ## Mean :0.007060 ## 3rd Qu.:0.007466 ## Max. :0.012427 ## NA&#39;s :12 ATE.TMLE.bounded1 &lt;- mean(ATE.TMLE.bounded.vector1, na.rm = TRUE) ATE.TMLE.bounded1 ## [1] 0.007059689 6.11 Step 8: Rescale effect estimate We make sure to transform back to our original scale. 6.11.1 \\(\\hat\\epsilon\\) = \\(\\hat\\epsilon_0\\) and \\(\\hat\\epsilon_1\\) ATE.TMLE &lt;- (max.Y-min.Y)*ATE.TMLE.bounded ATE.TMLE ## [1] 2.799122 6.11.2 Only 1 \\(\\hat\\epsilon\\) Alternatively, using H.AL: ATE.TMLE1 &lt;- (max.Y-min.Y)*ATE.TMLE.bounded1 ATE.TMLE1 ## [1] 2.767398 6.12 Step 9: Confidence interval estimation Since the machine learning algorithms were used only in intermediary steps, rather than estimating our parameter of interest directly, 95% confidence intervals can be calculated directly (Luque-Fernandez et al. 2018). Based on semi-parametric theory, closed form variance formula is already derived (Laan and Petersen 2012). Time-consuming bootstrap procedure is not necessary. ci.estimate &lt;- function(data = ObsData, H.AL.components = 1){ min.Y &lt;- min(data$Y) max.Y &lt;- max(data$Y) # transform predicted outcomes back to original scale if (H.AL.components == 2){ data$Pred.Y1.update.rescaled &lt;- (max.Y- min.Y)*data$Pred.Y1.update + min.Y data$Pred.Y0.update.rescaled &lt;- (max.Y- min.Y)*data$Pred.Y0.update + min.Y } if (H.AL.components == 1) { data$Pred.Y1.update.rescaled &lt;- (max.Y- min.Y)*data$Pred.Y1.update1 + min.Y data$Pred.Y0.update.rescaled &lt;- (max.Y- min.Y)*data$Pred.Y0.update1 + min.Y } EY1_TMLE1 &lt;- mean(data$Pred.Y1.update.rescaled, na.rm = TRUE) EY0_TMLE1 &lt;- mean(data$Pred.Y0.update.rescaled, na.rm = TRUE) # ATE efficient influence curve D1 &lt;- data$A/data$PS.SL* (data$Y - data$Pred.Y1.update.rescaled) + data$Pred.Y1.update.rescaled - EY1_TMLE1 D0 &lt;- (1 - data$A)/(1 - data$PS.SL)* (data$Y - data$Pred.Y0.update.rescaled) + data$Pred.Y0.update.rescaled - EY0_TMLE1 EIC &lt;- D1 - D0 # ATE variance n &lt;- nrow(data) varHat.IC &lt;- var(EIC, na.rm = TRUE)/n # ATE 95% CI if (H.AL.components == 2) { ATE.TMLE.CI &lt;- c(ATE.TMLE - 1.96*sqrt(varHat.IC), ATE.TMLE + 1.96*sqrt(varHat.IC)) } if (H.AL.components == 1) { ATE.TMLE.CI &lt;- c(ATE.TMLE1 - 1.96*sqrt(varHat.IC), ATE.TMLE1 + 1.96*sqrt(varHat.IC)) } return(ATE.TMLE.CI) } 6.12.1 \\(\\hat\\epsilon\\) = \\(\\hat\\epsilon_0\\) and \\(\\hat\\epsilon_1\\) ci.estimate(data = ObsData, H.AL.components = 2) ## [1] 1.818815 3.779430 6.12.2 Only 1 \\(\\hat\\epsilon\\) ci.estimate(data = ObsData, H.AL.components = 1) ## [1] 1.786472 3.748324 References "]]
